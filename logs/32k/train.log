2023-02-18 17:28:03,219	32k	INFO	{'train': {'log_interval': 200, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 12, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 17920, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 384, 'port': '8001'}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 2}, 'spk': {'dodo': 0}, 'model_dir': './logs\\32k'}
2023-02-18 17:28:06,932	32k	INFO	emb_g.weight is not in the checkpoint
2023-02-18 17:28:06,972	32k	INFO	Loaded checkpoint './logs\32k\G_0.pth' (iteration 1)
2023-02-18 17:28:08,479	32k	INFO	Loaded checkpoint './logs\32k\D_0.pth' (iteration 1)
2023-02-18 17:30:52,732	32k	INFO	{'train': {'log_interval': 200, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 12, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 17920, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 384, 'port': '8001'}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 2}, 'spk': {'dodo': 0}, 'model_dir': './logs\\32k'}
2023-02-18 17:30:53,952	32k	INFO	emb_g.weight is not in the checkpoint
2023-02-18 17:30:53,989	32k	INFO	Loaded checkpoint './logs\32k\G_0.pth' (iteration 1)
2023-02-18 17:30:54,056	32k	INFO	Loaded checkpoint './logs\32k\D_0.pth' (iteration 1)
2023-02-18 17:38:12,925	32k	INFO	{'train': {'log_interval': 200, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 12, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 17920, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 384, 'port': '8001'}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 2}, 'spk': {'dodo': 0}, 'model_dir': './logs\\32k'}
2023-02-18 17:38:14,180	32k	INFO	emb_g.weight is not in the checkpoint
2023-02-18 17:38:14,222	32k	INFO	Loaded checkpoint './logs\32k\G_0.pth' (iteration 1)
2023-02-18 17:38:14,293	32k	INFO	Loaded checkpoint './logs\32k\D_0.pth' (iteration 1)
2023-02-18 17:38:38,439	32k	INFO	Train Epoch: 1 [0%]
2023-02-18 17:38:38,439	32k	INFO	[2.6858417987823486, 2.8425562381744385, 5.799555778503418, 63.61255645751953, 19.00187110900879, 0, 0.0001]
2023-02-18 17:38:42,053	32k	INFO	Saving model and optimizer state at iteration 1 to ./logs\32k\G_0.pth
2023-02-18 17:38:43,884	32k	INFO	Saving model and optimizer state at iteration 1 to ./logs\32k\D_0.pth
2023-02-18 17:39:24,741	32k	INFO	====> Epoch: 1
2023-02-18 17:40:03,430	32k	INFO	====> Epoch: 2
2023-02-18 17:40:43,113	32k	INFO	====> Epoch: 3
2023-02-18 17:41:28,500	32k	INFO	====> Epoch: 4
2023-02-18 17:41:42,594	32k	INFO	Train Epoch: 5 [0%]
2023-02-18 17:41:42,595	32k	INFO	[2.060119390487671, 2.7058351039886475, 6.066415309906006, 18.21684455871582, 1.086459755897522, 200, 9.995000937421877e-05]
2023-02-18 17:42:09,012	32k	INFO	====> Epoch: 5
2023-02-18 17:42:56,402	32k	INFO	====> Epoch: 6
2023-02-18 17:43:44,152	32k	INFO	====> Epoch: 7
2023-02-18 17:44:30,960	32k	INFO	====> Epoch: 8
2023-02-18 17:44:46,543	32k	INFO	Train Epoch: 9 [0%]
2023-02-18 17:44:46,543	32k	INFO	[2.1243972778320312, 2.638392210006714, 6.271410942077637, 23.020877838134766, 0.7840297222137451, 400, 9.990004373906418e-05]
2023-02-18 17:45:17,202	32k	INFO	====> Epoch: 9
2023-02-18 17:46:06,072	32k	INFO	====> Epoch: 10
2023-02-18 17:46:54,403	32k	INFO	====> Epoch: 11
2023-02-18 17:47:35,258	32k	INFO	====> Epoch: 12
2023-02-18 17:47:49,438	32k	INFO	Train Epoch: 13 [0%]
2023-02-18 17:47:49,438	32k	INFO	[2.5799407958984375, 2.423351764678955, 4.647822380065918, 18.644020080566406, 1.0169644355773926, 600, 9.98501030820433e-05]
2023-02-18 17:48:23,661	32k	INFO	====> Epoch: 13
2023-02-18 17:49:11,809	32k	INFO	====> Epoch: 14
2023-02-18 17:49:59,332	32k	INFO	====> Epoch: 15
2023-02-18 17:50:46,715	32k	INFO	====> Epoch: 16
2023-02-18 17:51:01,144	32k	INFO	Train Epoch: 17 [0%]
2023-02-18 17:51:01,145	32k	INFO	[2.4737038612365723, 2.613023281097412, 5.418004035949707, 19.830659866333008, 0.7543134689331055, 800, 9.980018739066937e-05]
2023-02-18 17:51:34,960	32k	INFO	====> Epoch: 17
2023-02-18 17:52:22,750	32k	INFO	====> Epoch: 18
2023-02-18 17:53:10,631	32k	INFO	====> Epoch: 19
2023-02-18 17:53:58,477	32k	INFO	====> Epoch: 20
2023-02-18 17:54:12,688	32k	INFO	Train Epoch: 21 [0%]
2023-02-18 17:54:12,689	32k	INFO	[2.4730091094970703, 2.4859750270843506, 5.38751745223999, 20.962087631225586, 1.0289764404296875, 1000, 9.975029665246193e-05]
2023-02-18 17:54:15,688	32k	INFO	Saving model and optimizer state at iteration 21 to ./logs\32k\G_1000.pth
2023-02-18 17:54:18,244	32k	INFO	Saving model and optimizer state at iteration 21 to ./logs\32k\D_1000.pth
2023-02-18 17:54:56,495	32k	INFO	====> Epoch: 21
2023-02-18 17:55:44,831	32k	INFO	====> Epoch: 22
2023-02-18 17:56:32,835	32k	INFO	====> Epoch: 23
2023-02-18 17:57:21,297	32k	INFO	====> Epoch: 24
2023-02-18 17:57:35,862	32k	INFO	Train Epoch: 25 [0%]
2023-02-18 17:57:35,862	32k	INFO	[2.37953782081604, 2.2821640968322754, 5.2566728591918945, 20.705169677734375, 1.0220775604248047, 1200, 9.970043085494672e-05]
2023-02-18 17:58:09,390	32k	INFO	====> Epoch: 25
2023-02-18 17:58:56,973	32k	INFO	====> Epoch: 26
2023-02-18 17:59:44,640	32k	INFO	====> Epoch: 27
2023-02-18 18:00:32,246	32k	INFO	====> Epoch: 28
2023-02-18 18:00:46,392	32k	INFO	Train Epoch: 29 [0%]
2023-02-18 18:00:46,392	32k	INFO	[2.411787748336792, 2.331369638442993, 4.795619010925293, 18.540319442749023, 0.9767314195632935, 1400, 9.965058998565574e-05]
2023-02-18 18:01:20,156	32k	INFO	====> Epoch: 29
2023-02-18 18:02:08,577	32k	INFO	====> Epoch: 30
2023-02-18 18:02:56,555	32k	INFO	====> Epoch: 31
2023-02-18 18:03:40,122	32k	INFO	====> Epoch: 32
2023-02-18 18:03:54,503	32k	INFO	Train Epoch: 33 [0%]
2023-02-18 18:03:54,503	32k	INFO	[2.555912494659424, 2.167280912399292, 4.578885555267334, 19.676063537597656, 0.7649874091148376, 1600, 9.960077403212722e-05]
2023-02-18 18:04:28,683	32k	INFO	====> Epoch: 33
2023-02-18 18:05:17,043	32k	INFO	====> Epoch: 34
2023-02-18 18:06:04,734	32k	INFO	====> Epoch: 35
2023-02-18 18:06:52,173	32k	INFO	====> Epoch: 36
2023-02-18 18:07:06,444	32k	INFO	Train Epoch: 37 [0%]
2023-02-18 18:07:06,444	32k	INFO	[2.470156192779541, 2.0787158012390137, 4.648595333099365, 17.520503997802734, 1.2680774927139282, 1800, 9.95509829819056e-05]
2023-02-18 18:07:40,314	32k	INFO	====> Epoch: 37
2023-02-18 18:08:25,414	32k	INFO	====> Epoch: 38
2023-02-18 18:09:13,042	32k	INFO	====> Epoch: 39
2023-02-18 18:10:01,106	32k	INFO	====> Epoch: 40
2023-02-18 18:10:15,629	32k	INFO	Train Epoch: 41 [0%]
2023-02-18 18:10:15,629	32k	INFO	[2.4612057209014893, 2.3029286861419678, 4.886150360107422, 20.847074508666992, 1.0200835466384888, 2000, 9.950121682254156e-05]
2023-02-18 18:10:18,512	32k	INFO	Saving model and optimizer state at iteration 41 to ./logs\32k\G_2000.pth
2023-02-18 18:10:20,632	32k	INFO	Saving model and optimizer state at iteration 41 to ./logs\32k\D_2000.pth
2023-02-18 18:10:51,593	32k	INFO	====> Epoch: 41
2023-02-18 18:11:39,802	32k	INFO	====> Epoch: 42
2023-02-18 18:12:23,786	32k	INFO	====> Epoch: 43
2023-02-18 18:13:11,708	32k	INFO	====> Epoch: 44
2023-02-18 18:13:26,079	32k	INFO	Train Epoch: 45 [0%]
2023-02-18 18:13:26,080	32k	INFO	[2.5274364948272705, 2.160466194152832, 4.295843601226807, 16.995792388916016, 0.8307552933692932, 2200, 9.945147554159202e-05]
2023-02-18 18:13:59,667	32k	INFO	====> Epoch: 45
2023-02-18 18:14:47,084	32k	INFO	====> Epoch: 46
2023-02-18 18:15:34,477	32k	INFO	====> Epoch: 47
2023-02-18 18:16:22,201	32k	INFO	====> Epoch: 48
2023-02-18 18:16:36,499	32k	INFO	Train Epoch: 49 [0%]
2023-02-18 18:16:36,500	32k	INFO	[2.4491968154907227, 2.306483507156372, 5.213939189910889, 21.100093841552734, 0.8060850501060486, 2400, 9.940175912662009e-05]
2023-02-18 18:17:10,283	32k	INFO	====> Epoch: 49
2023-02-18 18:17:58,296	32k	INFO	====> Epoch: 50
2023-02-18 18:18:46,374	32k	INFO	====> Epoch: 51
2023-02-18 18:19:34,162	32k	INFO	====> Epoch: 52
2023-02-18 18:19:48,230	32k	INFO	Train Epoch: 53 [0%]
2023-02-18 18:19:48,231	32k	INFO	[2.426844596862793, 2.314807415008545, 4.976682186126709, 18.262094497680664, 0.9246633052825928, 2600, 9.935206756519513e-05]
2023-02-18 18:20:21,992	32k	INFO	====> Epoch: 53
2023-02-18 18:21:09,975	32k	INFO	====> Epoch: 54
2023-02-18 18:21:57,398	32k	INFO	====> Epoch: 55
2023-02-18 18:22:45,047	32k	INFO	====> Epoch: 56
2023-02-18 18:22:59,346	32k	INFO	Train Epoch: 57 [0%]
2023-02-18 18:22:59,347	32k	INFO	[2.4311861991882324, 2.216423749923706, 4.8563995361328125, 18.483905792236328, 0.8589459657669067, 2800, 9.930240084489267e-05]
2023-02-18 18:23:32,741	32k	INFO	====> Epoch: 57
2023-02-18 18:24:20,470	32k	INFO	====> Epoch: 58
2023-02-18 18:25:08,132	32k	INFO	====> Epoch: 59
2023-02-18 18:25:55,940	32k	INFO	====> Epoch: 60
2023-02-18 18:26:10,241	32k	INFO	Train Epoch: 61 [0%]
2023-02-18 18:26:10,242	32k	INFO	[2.407072067260742, 2.298166275024414, 5.375918865203857, 20.816574096679688, 0.8406436443328857, 3000, 9.92527589532945e-05]
2023-02-18 18:26:13,190	32k	INFO	Saving model and optimizer state at iteration 61 to ./logs\32k\G_3000.pth
2023-02-18 18:26:15,477	32k	INFO	Saving model and optimizer state at iteration 61 to ./logs\32k\D_3000.pth
2023-02-18 18:26:54,971	32k	INFO	====> Epoch: 61
2023-02-18 18:27:43,250	32k	INFO	====> Epoch: 62
2023-02-18 18:28:31,263	32k	INFO	====> Epoch: 63
2023-02-18 18:29:19,408	32k	INFO	====> Epoch: 64
2023-02-18 18:29:33,832	32k	INFO	Train Epoch: 65 [0%]
2023-02-18 18:29:33,833	32k	INFO	[2.5561132431030273, 2.0606417655944824, 4.249897003173828, 15.7490873336792, 0.666767418384552, 3200, 9.92031418779886e-05]
2023-02-18 18:30:07,423	32k	INFO	====> Epoch: 65
2023-02-18 18:30:55,023	32k	INFO	====> Epoch: 66
2023-02-18 18:31:43,492	32k	INFO	====> Epoch: 67
2023-02-18 18:32:31,030	32k	INFO	====> Epoch: 68
2023-02-18 18:32:45,337	32k	INFO	Train Epoch: 69 [0%]
2023-02-18 18:32:45,337	32k	INFO	[2.4875266551971436, 2.246908187866211, 4.784417629241943, 18.93488121032715, 0.7356613278388977, 3400, 9.915354960656915e-05]
2023-02-18 18:33:19,076	32k	INFO	====> Epoch: 69
2023-02-18 18:34:06,933	32k	INFO	====> Epoch: 70
2023-02-18 18:34:54,481	32k	INFO	====> Epoch: 71
2023-02-18 18:35:42,363	32k	INFO	====> Epoch: 72
2023-02-18 18:35:56,899	32k	INFO	Train Epoch: 73 [0%]
2023-02-18 18:35:56,899	32k	INFO	[2.458232879638672, 2.3075108528137207, 4.486081600189209, 17.134340286254883, 0.7370619773864746, 3600, 9.910398212663652e-05]
2023-02-18 18:36:31,047	32k	INFO	====> Epoch: 73
2023-02-18 18:37:19,138	32k	INFO	====> Epoch: 74
2023-02-18 18:38:06,726	32k	INFO	====> Epoch: 75
2023-02-18 18:38:53,866	32k	INFO	====> Epoch: 76
2023-02-18 18:39:08,217	32k	INFO	Train Epoch: 77 [0%]
2023-02-18 18:39:08,217	32k	INFO	[2.4893388748168945, 2.276153564453125, 4.542057514190674, 15.498451232910156, 0.7817887663841248, 3800, 9.905443942579728e-05]
2023-02-18 18:39:41,904	32k	INFO	====> Epoch: 77
2023-02-18 18:40:29,864	32k	INFO	====> Epoch: 78
2023-02-18 18:41:15,379	32k	INFO	====> Epoch: 79
2023-02-18 18:42:01,946	32k	INFO	====> Epoch: 80
2023-02-18 18:42:15,930	32k	INFO	Train Epoch: 81 [0%]
2023-02-18 18:42:15,931	32k	INFO	[2.416388511657715, 2.18825101852417, 4.551788330078125, 16.948884963989258, 0.6887826919555664, 4000, 9.900492149166423e-05]
2023-02-18 18:42:18,707	32k	INFO	Saving model and optimizer state at iteration 81 to ./logs\32k\G_4000.pth
2023-02-18 18:42:20,975	32k	INFO	Saving model and optimizer state at iteration 81 to ./logs\32k\D_4000.pth
2023-02-18 18:42:51,586	32k	INFO	====> Epoch: 81
2023-02-18 18:43:40,307	32k	INFO	====> Epoch: 82
2023-02-18 18:44:27,246	32k	INFO	====> Epoch: 83
2023-02-18 18:45:07,174	32k	INFO	====> Epoch: 84
2023-02-18 18:45:21,464	32k	INFO	Train Epoch: 85 [0%]
2023-02-18 18:45:21,465	32k	INFO	[2.3924801349639893, 2.391075611114502, 4.506153583526611, 16.440507888793945, 0.5762032866477966, 4200, 9.895542831185631e-05]
2023-02-18 18:45:55,680	32k	INFO	====> Epoch: 85
2023-02-18 18:46:43,981	32k	INFO	====> Epoch: 86
2023-02-18 18:47:32,047	32k	INFO	====> Epoch: 87
2023-02-18 18:48:20,072	32k	INFO	====> Epoch: 88
2023-02-18 18:48:34,424	32k	INFO	Train Epoch: 89 [0%]
2023-02-18 18:48:34,425	32k	INFO	[2.288849353790283, 2.29775333404541, 4.899404048919678, 16.96706199645996, 0.8072628378868103, 4400, 9.89059598739987e-05]
2023-02-18 18:49:08,150	32k	INFO	====> Epoch: 89
2023-02-18 18:49:56,918	32k	INFO	====> Epoch: 90
2023-02-18 18:50:45,245	32k	INFO	====> Epoch: 91
2023-02-18 18:51:33,563	32k	INFO	====> Epoch: 92
2023-02-18 18:51:47,802	32k	INFO	Train Epoch: 93 [0%]
2023-02-18 18:51:47,802	32k	INFO	[2.2865207195281982, 2.1257736682891846, 4.869661331176758, 17.583162307739258, 0.8291712999343872, 4600, 9.885651616572276e-05]
2023-02-18 18:52:21,929	32k	INFO	====> Epoch: 93
2023-02-18 18:53:10,368	32k	INFO	====> Epoch: 94
2023-02-18 18:53:56,037	32k	INFO	====> Epoch: 95
2023-02-18 18:54:44,238	32k	INFO	====> Epoch: 96
2023-02-18 18:54:58,940	32k	INFO	Train Epoch: 97 [0%]
2023-02-18 18:54:58,941	32k	INFO	[2.5883922576904297, 2.123765468597412, 3.891033172607422, 14.726727485656738, 0.771423876285553, 4800, 9.880709717466598e-05]
2023-02-18 18:55:29,665	32k	INFO	====> Epoch: 97
2023-02-18 18:56:09,529	32k	INFO	====> Epoch: 98
2023-02-18 18:56:58,427	32k	INFO	====> Epoch: 99
2023-02-18 18:57:47,634	32k	INFO	====> Epoch: 100
2023-02-18 18:58:02,463	32k	INFO	Train Epoch: 101 [0%]
2023-02-18 18:58:02,463	32k	INFO	[2.6160783767700195, 2.14786958694458, 4.165035724639893, 17.42580795288086, 0.4906075894832611, 5000, 9.875770288847208e-05]
2023-02-18 18:58:05,715	32k	INFO	Saving model and optimizer state at iteration 101 to ./logs\32k\G_5000.pth
2023-02-18 18:58:08,951	32k	INFO	Saving model and optimizer state at iteration 101 to ./logs\32k\D_5000.pth
2023-02-18 18:58:48,409	32k	INFO	====> Epoch: 101
2023-02-18 18:59:37,588	32k	INFO	====> Epoch: 102
2023-02-18 19:00:26,448	32k	INFO	====> Epoch: 103
2023-02-18 19:01:13,122	32k	INFO	====> Epoch: 104
2023-02-18 19:01:27,784	32k	INFO	Train Epoch: 105 [0%]
2023-02-18 19:01:27,784	32k	INFO	[2.4598021507263184, 2.2545666694641113, 4.213431358337402, 15.29686164855957, 0.70118647813797, 5200, 9.870833329479095e-05]
2023-02-18 19:02:02,256	32k	INFO	====> Epoch: 105
2023-02-18 19:02:49,002	32k	INFO	====> Epoch: 106
2023-02-18 19:03:37,340	32k	INFO	====> Epoch: 107
2023-02-18 19:04:26,226	32k	INFO	====> Epoch: 108
2023-02-18 19:04:41,337	32k	INFO	Train Epoch: 109 [0%]
2023-02-18 19:04:41,338	32k	INFO	[2.395007848739624, 2.297065496444702, 4.937794208526611, 18.133216857910156, 0.8526710867881775, 5400, 9.865898838127865e-05]
2023-02-18 19:05:15,539	32k	INFO	====> Epoch: 109
2023-02-18 19:06:04,406	32k	INFO	====> Epoch: 110
2023-02-18 19:06:53,520	32k	INFO	====> Epoch: 111
2023-02-18 19:07:42,241	32k	INFO	====> Epoch: 112
2023-02-18 19:07:57,251	32k	INFO	Train Epoch: 113 [0%]
2023-02-18 19:07:57,251	32k	INFO	[2.4390528202056885, 2.216291904449463, 5.347744941711426, 18.449054718017578, 0.8107398152351379, 5600, 9.86096681355974e-05]
2023-02-18 19:08:31,655	32k	INFO	====> Epoch: 113
2023-02-18 19:09:20,555	32k	INFO	====> Epoch: 114
2023-02-18 19:10:08,912	32k	INFO	====> Epoch: 115
2023-02-18 19:10:57,107	32k	INFO	====> Epoch: 116
2023-02-18 19:11:11,838	32k	INFO	Train Epoch: 117 [0%]
2023-02-18 19:11:11,839	32k	INFO	[2.4979615211486816, 2.089824914932251, 4.67603874206543, 17.22005271911621, 0.8140923380851746, 5800, 9.85603725454156e-05]
2023-02-18 19:11:43,200	32k	INFO	====> Epoch: 117
2023-02-18 19:12:28,001	32k	INFO	====> Epoch: 118
2023-02-18 19:13:15,175	32k	INFO	====> Epoch: 119
2023-02-18 19:14:03,206	32k	INFO	====> Epoch: 120
2023-02-18 19:14:17,917	32k	INFO	Train Epoch: 121 [0%]
2023-02-18 19:14:17,917	32k	INFO	[2.407078981399536, 2.3779256343841553, 4.615982532501221, 17.342491149902344, 0.5289373993873596, 6000, 9.851110159840781e-05]
2023-02-18 19:14:20,895	32k	INFO	Saving model and optimizer state at iteration 121 to ./logs\32k\G_6000.pth
2023-02-18 19:14:23,494	32k	INFO	Saving model and optimizer state at iteration 121 to ./logs\32k\D_6000.pth
2023-02-18 19:15:04,009	32k	INFO	====> Epoch: 121
2023-02-18 19:15:52,065	32k	INFO	====> Epoch: 122
2023-02-18 19:16:37,976	32k	INFO	====> Epoch: 123
2023-02-18 19:17:24,877	32k	INFO	====> Epoch: 124
2023-02-18 19:17:38,869	32k	INFO	Train Epoch: 125 [0%]
2023-02-18 19:17:38,869	32k	INFO	[2.2917592525482178, 2.228062152862549, 5.519164085388184, 21.45032501220703, 0.6496487855911255, 6200, 9.846185528225477e-05]
2023-02-18 19:18:11,911	32k	INFO	====> Epoch: 125
2023-02-18 19:19:00,280	32k	INFO	====> Epoch: 126
2023-02-18 19:27:44,023	32k	INFO	{'train': {'log_interval': 200, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 12, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 17920, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 384, 'port': '8001'}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 2}, 'spk': {'dodo': 0}, 'model_dir': './logs\\32k'}
2023-02-18 19:27:45,736	32k	INFO	Loaded checkpoint './logs\32k\G_6000.pth' (iteration 121)
2023-02-18 19:27:46,032	32k	INFO	Loaded checkpoint './logs\32k\D_6000.pth' (iteration 121)
2023-02-18 19:28:05,226	32k	INFO	Train Epoch: 121 [0%]
2023-02-18 19:28:05,227	32k	INFO	[2.51243257522583, 2.324807643890381, 4.577277660369873, 18.293575286865234, 0.7451529502868652, 6000, 9.8498787710708e-05]
2023-02-18 19:28:08,759	32k	INFO	Saving model and optimizer state at iteration 121 to ./logs\32k\G_6000.pth
2023-02-18 19:28:09,854	32k	INFO	Saving model and optimizer state at iteration 121 to ./logs\32k\D_6000.pth
2023-02-18 19:28:48,143	32k	INFO	====> Epoch: 121
2023-02-18 19:29:36,708	32k	INFO	====> Epoch: 122
2023-02-18 19:30:26,910	32k	INFO	====> Epoch: 123
2023-02-18 19:31:16,332	32k	INFO	====> Epoch: 124
2023-02-18 19:31:30,934	32k	INFO	Train Epoch: 125 [0%]
2023-02-18 19:31:30,934	32k	INFO	[2.5693321228027344, 2.2505009174346924, 4.1283674240112305, 14.689407348632812, 0.8505565524101257, 6200, 9.84495475503445e-05]
2023-02-18 19:32:05,192	32k	INFO	====> Epoch: 125
2023-02-18 19:32:53,742	32k	INFO	====> Epoch: 126
2023-02-18 19:33:41,978	32k	INFO	====> Epoch: 127
2023-02-18 19:34:30,396	32k	INFO	====> Epoch: 128
2023-02-18 19:34:45,261	32k	INFO	Train Epoch: 129 [0%]
2023-02-18 19:34:45,262	32k	INFO	[2.4244863986968994, 2.2212111949920654, 5.4254350662231445, 19.729705810546875, 0.5401251912117004, 6400, 9.840033200544528e-05]
2023-02-18 19:35:19,525	32k	INFO	====> Epoch: 129
2023-02-18 19:36:07,469	32k	INFO	====> Epoch: 130
2023-02-18 19:36:50,516	32k	INFO	====> Epoch: 131
2023-02-18 19:37:31,379	32k	INFO	====> Epoch: 132
2023-02-18 19:37:46,036	32k	INFO	Train Epoch: 133 [0%]
2023-02-18 19:37:46,036	32k	INFO	[2.5020699501037598, 2.1781322956085205, 4.242824554443359, 16.456857681274414, 0.840949296951294, 6600, 9.835114106370493e-05]
2023-02-18 19:38:11,655	32k	INFO	====> Epoch: 133
2023-02-18 19:38:54,282	32k	INFO	====> Epoch: 134
2023-02-18 19:39:34,591	32k	INFO	====> Epoch: 135
2023-02-18 19:40:20,206	32k	INFO	====> Epoch: 136
2023-02-18 19:40:36,316	32k	INFO	Train Epoch: 137 [0%]
2023-02-18 19:40:36,317	32k	INFO	[2.5172839164733887, 2.2403364181518555, 4.606062889099121, 17.474273681640625, 0.6061837673187256, 6800, 9.830197471282419e-05]
2023-02-18 19:41:04,354	32k	INFO	====> Epoch: 137
2023-02-18 19:41:47,758	32k	INFO	====> Epoch: 138
2023-02-18 19:42:29,174	32k	INFO	====> Epoch: 139
2023-02-18 19:43:09,776	32k	INFO	====> Epoch: 140
2023-02-18 19:43:24,592	32k	INFO	Train Epoch: 141 [0%]
2023-02-18 19:43:24,592	32k	INFO	[2.532212257385254, 2.3479020595550537, 5.175135135650635, 19.00993537902832, 0.8738209009170532, 7000, 9.825283294050992e-05]
2023-02-18 19:43:27,773	32k	INFO	Saving model and optimizer state at iteration 141 to ./logs\32k\G_7000.pth
2023-02-18 19:43:30,073	32k	INFO	Saving model and optimizer state at iteration 141 to ./logs\32k\D_7000.pth
2023-02-18 19:44:17,152	32k	INFO	====> Epoch: 141
2023-02-18 19:44:59,939	32k	INFO	====> Epoch: 142
2023-02-18 19:45:48,053	32k	INFO	====> Epoch: 143
2023-02-18 19:46:35,898	32k	INFO	====> Epoch: 144
2023-02-18 19:46:50,793	32k	INFO	Train Epoch: 145 [0%]
2023-02-18 19:46:50,793	32k	INFO	[2.3939414024353027, 2.2910213470458984, 4.941467761993408, 18.572154998779297, 0.7712522149085999, 7200, 9.820371573447515e-05]
2023-02-18 19:47:26,991	32k	INFO	====> Epoch: 145
2023-02-18 19:48:15,617	32k	INFO	====> Epoch: 146
2023-02-18 19:49:02,346	32k	INFO	====> Epoch: 147
2023-02-18 19:49:52,402	32k	INFO	====> Epoch: 148
2023-02-18 19:50:07,278	32k	INFO	Train Epoch: 149 [0%]
2023-02-18 19:50:07,278	32k	INFO	[2.492330312728882, 2.1276023387908936, 4.686033248901367, 17.2503604888916, 0.7946329712867737, 7400, 9.815462308243906e-05]
2023-02-18 19:50:42,188	32k	INFO	====> Epoch: 149
2023-02-18 19:51:31,973	32k	INFO	====> Epoch: 150
2023-02-18 19:52:12,774	32k	INFO	====> Epoch: 151
2023-02-18 19:52:54,104	32k	INFO	====> Epoch: 152
2023-02-18 19:53:09,263	32k	INFO	Train Epoch: 153 [0%]
2023-02-18 19:53:09,264	32k	INFO	[2.497009754180908, 2.3064522743225098, 4.647298336029053, 18.4637451171875, 0.7212376594543457, 7600, 9.810555497212693e-05]
2023-02-18 19:53:38,915	32k	INFO	====> Epoch: 153
2023-02-18 19:54:24,512	32k	INFO	====> Epoch: 154
2023-02-18 19:55:14,685	32k	INFO	====> Epoch: 155
2023-02-18 19:55:59,319	32k	INFO	====> Epoch: 156
2023-02-18 19:56:14,177	32k	INFO	Train Epoch: 157 [0%]
2023-02-18 19:56:14,177	32k	INFO	[2.568669080734253, 2.2153165340423584, 4.744801044464111, 16.46042251586914, 1.0899345874786377, 7800, 9.80565113912702e-05]
2023-02-18 19:56:41,148	32k	INFO	====> Epoch: 157
2023-02-18 19:57:30,422	32k	INFO	====> Epoch: 158
2023-02-18 19:58:20,612	32k	INFO	====> Epoch: 159
2023-02-18 19:59:10,530	32k	INFO	====> Epoch: 160
2023-02-18 19:59:25,287	32k	INFO	Train Epoch: 161 [0%]
2023-02-18 19:59:25,287	32k	INFO	[2.586160898208618, 2.244108200073242, 4.6755595207214355, 19.417848587036133, 0.8388677835464478, 8000, 9.800749232760646e-05]
2023-02-18 19:59:28,453	32k	INFO	Saving model and optimizer state at iteration 161 to ./logs\32k\G_8000.pth
2023-02-18 19:59:29,986	32k	INFO	Saving model and optimizer state at iteration 161 to ./logs\32k\D_8000.pth
2023-02-18 20:00:15,280	32k	INFO	====> Epoch: 161
2023-02-18 20:00:57,768	32k	INFO	====> Epoch: 162
2023-02-18 20:01:44,132	32k	INFO	====> Epoch: 163
2023-02-18 20:02:23,864	32k	INFO	====> Epoch: 164
2023-02-18 20:02:38,565	32k	INFO	Train Epoch: 165 [0%]
2023-02-18 20:02:38,565	32k	INFO	[2.5252928733825684, 2.131467580795288, 4.394660472869873, 16.135292053222656, 0.5883500576019287, 8200, 9.795849776887939e-05]
2023-02-18 20:03:12,533	32k	INFO	====> Epoch: 165
2023-02-18 20:04:00,274	32k	INFO	====> Epoch: 166
2023-02-18 20:04:44,070	32k	INFO	====> Epoch: 167
2023-02-18 20:05:32,957	32k	INFO	====> Epoch: 168
2023-02-18 20:05:47,745	32k	INFO	Train Epoch: 169 [0%]
2023-02-18 20:05:47,745	32k	INFO	[2.3671326637268066, 2.197354793548584, 5.343832492828369, 19.80169677734375, 0.6843068599700928, 8400, 9.790952770283884e-05]
2023-02-18 20:06:21,786	32k	INFO	====> Epoch: 169
2023-02-18 20:07:04,853	32k	INFO	====> Epoch: 170
2023-02-18 20:07:45,392	32k	INFO	====> Epoch: 171
2023-02-18 20:08:27,352	32k	INFO	====> Epoch: 172
2023-02-18 20:08:41,927	32k	INFO	Train Epoch: 173 [0%]
2023-02-18 20:08:41,928	32k	INFO	[2.3706159591674805, 2.3176863193511963, 5.056931018829346, 17.37143325805664, 0.8050429821014404, 8600, 9.786058211724074e-05]
2023-02-18 20:09:16,246	32k	INFO	====> Epoch: 173
2023-02-18 20:10:04,994	32k	INFO	====> Epoch: 174
2023-02-18 20:10:53,513	32k	INFO	====> Epoch: 175
2023-02-18 20:11:42,263	32k	INFO	====> Epoch: 176
2023-02-18 20:11:57,473	32k	INFO	Train Epoch: 177 [0%]
2023-02-18 20:11:57,473	32k	INFO	[2.4249837398529053, 2.1949987411499023, 4.999621868133545, 17.419519424438477, 0.7288433313369751, 8800, 9.781166099984716e-05]
2023-02-18 20:12:32,203	32k	INFO	====> Epoch: 177
2023-02-18 20:13:21,242	32k	INFO	====> Epoch: 178
2023-02-18 20:14:09,681	32k	INFO	====> Epoch: 179
2023-02-18 20:14:58,307	32k	INFO	====> Epoch: 180
2023-02-18 20:15:12,805	32k	INFO	Train Epoch: 181 [0%]
2023-02-18 20:15:12,806	32k	INFO	[2.4212489128112793, 2.2762529850006104, 5.3097076416015625, 19.449337005615234, 0.7397462129592896, 9000, 9.776276433842631e-05]
2023-02-18 20:15:15,857	32k	INFO	Saving model and optimizer state at iteration 181 to ./logs\32k\G_9000.pth
2023-02-18 20:15:17,493	32k	INFO	Saving model and optimizer state at iteration 181 to ./logs\32k\D_9000.pth
2023-02-18 20:15:59,954	32k	INFO	====> Epoch: 181
2023-02-18 20:16:45,648	32k	INFO	====> Epoch: 182
2023-02-18 20:17:34,488	32k	INFO	====> Epoch: 183
2023-02-18 20:18:18,081	32k	INFO	====> Epoch: 184
2023-02-18 20:18:32,752	32k	INFO	Train Epoch: 185 [0%]
2023-02-18 20:18:32,752	32k	INFO	[2.5437259674072266, 2.0230841636657715, 4.2090911865234375, 15.016484260559082, 0.5266954898834229, 9200, 9.771389212075249e-05]
2023-02-18 20:19:03,826	32k	INFO	====> Epoch: 185
2023-02-18 20:19:46,974	32k	INFO	====> Epoch: 186
2023-02-18 20:20:32,412	32k	INFO	====> Epoch: 187
2023-02-18 20:21:14,152	32k	INFO	====> Epoch: 188
2023-02-18 20:21:28,607	32k	INFO	Train Epoch: 189 [0%]
2023-02-18 20:21:28,608	32k	INFO	[2.4160852432250977, 2.1983470916748047, 4.9034905433654785, 18.068443298339844, 0.685584545135498, 9400, 9.766504433460612e-05]
2023-02-18 20:21:56,395	32k	INFO	====> Epoch: 189
2023-02-18 20:22:39,330	32k	INFO	====> Epoch: 190
2023-02-18 20:23:19,346	32k	INFO	====> Epoch: 191
2023-02-18 20:24:08,011	32k	INFO	====> Epoch: 192
2023-02-18 20:24:22,636	32k	INFO	Train Epoch: 193 [0%]
2023-02-18 20:24:22,637	32k	INFO	[2.4423108100891113, 2.3215222358703613, 4.51046085357666, 16.24523162841797, 0.6337330937385559, 9600, 9.761622096777372e-05]
2023-02-18 20:24:55,741	32k	INFO	====> Epoch: 193
2023-02-18 20:25:44,400	32k	INFO	====> Epoch: 194
2023-02-18 20:26:33,337	32k	INFO	====> Epoch: 195
2023-02-18 20:27:21,732	32k	INFO	====> Epoch: 196
2023-02-18 20:27:36,501	32k	INFO	Train Epoch: 197 [0%]
2023-02-18 20:27:36,501	32k	INFO	[2.454239845275879, 2.209221601486206, 4.422660827636719, 14.97575855255127, 0.6745627522468567, 9800, 9.756742200804793e-05]
2023-02-18 20:28:11,009	32k	INFO	====> Epoch: 197
2023-02-18 20:28:59,516	32k	INFO	====> Epoch: 198
2023-02-18 20:29:47,877	32k	INFO	====> Epoch: 199
2023-02-18 20:30:36,672	32k	INFO	====> Epoch: 200
2023-02-18 20:30:51,031	32k	INFO	Train Epoch: 201 [0%]
2023-02-18 20:30:51,032	32k	INFO	[2.481909990310669, 2.182220935821533, 4.448923110961914, 16.22548484802246, 0.5730075240135193, 10000, 9.75186474432275e-05]
2023-02-18 20:30:54,081	32k	INFO	Saving model and optimizer state at iteration 201 to ./logs\32k\G_10000.pth
2023-02-18 20:30:55,190	32k	INFO	Saving model and optimizer state at iteration 201 to ./logs\32k\D_10000.pth
2023-02-18 20:31:37,701	32k	INFO	====> Epoch: 201
2023-02-18 21:14:54,632	32k	INFO	{'train': {'log_interval': 200, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 12, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 17920, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 384, 'port': '8001'}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 2}, 'spk': {'dodo': 0}, 'model_dir': './logs\\32k'}
2023-02-18 21:14:56,296	32k	INFO	Loaded checkpoint './logs\32k\G_10000.pth' (iteration 201)
2023-02-18 21:14:56,584	32k	INFO	Loaded checkpoint './logs\32k\D_10000.pth' (iteration 201)
2023-02-18 21:15:15,482	32k	INFO	Train Epoch: 201 [0%]
2023-02-18 21:15:15,482	32k	INFO	[2.5078470706939697, 2.2121784687042236, 4.454307556152344, 17.889448165893555, 0.6802908778190613, 10000, 9.750645761229709e-05]
2023-02-18 21:15:19,152	32k	INFO	Saving model and optimizer state at iteration 201 to ./logs\32k\G_10000.pth
2023-02-18 21:15:22,819	32k	INFO	Saving model and optimizer state at iteration 201 to ./logs\32k\D_10000.pth
2023-02-18 21:15:54,972	32k	INFO	====> Epoch: 201
2023-02-18 21:16:35,189	32k	INFO	====> Epoch: 202
2023-02-18 21:17:16,599	32k	INFO	====> Epoch: 203
2023-02-18 21:17:55,885	32k	INFO	====> Epoch: 204
2023-02-18 21:18:09,979	32k	INFO	Train Epoch: 205 [0%]
2023-02-18 21:18:09,980	32k	INFO	[2.4431583881378174, 2.2488224506378174, 4.3543853759765625, 14.406208038330078, 0.7705065608024597, 10200, 9.745771352395957e-05]
2023-02-18 21:18:39,617	32k	INFO	====> Epoch: 205
2023-02-18 21:19:26,972	32k	INFO	====> Epoch: 206
2023-02-18 21:20:05,753	32k	INFO	====> Epoch: 207
2023-02-18 21:20:43,407	32k	INFO	====> Epoch: 208
2023-02-18 21:20:57,140	32k	INFO	Train Epoch: 209 [0%]
2023-02-18 21:20:57,141	32k	INFO	[2.3586931228637695, 2.293768882751465, 5.381887912750244, 19.321195602416992, 0.47563302516937256, 10400, 9.740899380309685e-05]
2023-02-18 21:21:26,904	32k	INFO	====> Epoch: 209
2023-02-18 21:22:08,664	32k	INFO	====> Epoch: 210
2023-02-18 21:22:53,479	32k	INFO	====> Epoch: 211
2023-02-18 21:23:37,069	32k	INFO	====> Epoch: 212
2023-02-18 21:23:51,788	32k	INFO	Train Epoch: 213 [0%]
2023-02-18 21:23:51,788	32k	INFO	[2.5440988540649414, 2.1780779361724854, 4.179599285125732, 16.19811248779297, 0.8177300691604614, 10600, 9.736029843752747e-05]
2023-02-18 21:24:18,576	32k	INFO	====> Epoch: 213
2023-02-18 21:25:01,902	32k	INFO	====> Epoch: 214
2023-02-18 21:25:45,015	32k	INFO	====> Epoch: 215
2023-02-18 21:26:27,236	32k	INFO	====> Epoch: 216
2023-02-18 21:26:41,961	32k	INFO	Train Epoch: 217 [0%]
2023-02-18 21:26:41,962	32k	INFO	[2.502678155899048, 2.414602279663086, 4.699206352233887, 17.199058532714844, 0.5673601627349854, 10800, 9.731162741507607e-05]
2023-02-18 21:27:10,209	32k	INFO	====> Epoch: 217
2023-02-18 21:27:56,781	32k	INFO	====> Epoch: 218
2023-02-18 21:28:38,192	32k	INFO	====> Epoch: 219
2023-02-18 21:29:21,196	32k	INFO	====> Epoch: 220
2023-02-18 21:29:35,724	32k	INFO	Train Epoch: 221 [0%]
2023-02-18 21:29:35,725	32k	INFO	[2.4755423069000244, 2.377337694168091, 5.272262096405029, 18.887226104736328, 0.8085229992866516, 11000, 9.726298072357337e-05]
2023-02-18 21:29:38,790	32k	INFO	Saving model and optimizer state at iteration 221 to ./logs\32k\G_11000.pth
2023-02-18 21:29:40,306	32k	INFO	Saving model and optimizer state at iteration 221 to ./logs\32k\D_11000.pth
2023-02-18 21:30:15,129	32k	INFO	====> Epoch: 221
2023-02-18 23:23:37,248	32k	INFO	{'train': {'log_interval': 200, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 12, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 17920, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 384, 'port': '8001'}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 2}, 'spk': {'dodo': 0}, 'model_dir': './logs\\32k'}
2023-02-18 23:23:38,880	32k	INFO	Loaded checkpoint './logs\32k\G_11000.pth' (iteration 221)
2023-02-18 23:23:39,134	32k	INFO	Loaded checkpoint './logs\32k\D_11000.pth' (iteration 221)
2023-02-18 23:23:56,475	32k	INFO	Train Epoch: 221 [0%]
2023-02-18 23:23:56,475	32k	INFO	[2.4500131607055664, 2.329148054122925, 4.60134744644165, 17.824420928955078, 0.6016842722892761, 11000, 9.725082285098293e-05]
2023-02-18 23:24:00,329	32k	INFO	Saving model and optimizer state at iteration 221 to ./logs\32k\G_11000.pth
2023-02-18 23:24:01,001	32k	INFO	Saving model and optimizer state at iteration 221 to ./logs\32k\D_11000.pth
2023-02-18 23:24:30,494	32k	INFO	====> Epoch: 221
2023-02-18 23:25:04,543	32k	INFO	====> Epoch: 222
2023-02-18 23:25:38,726	32k	INFO	====> Epoch: 223
2023-02-18 23:26:13,315	32k	INFO	====> Epoch: 224
2023-02-18 23:26:26,008	32k	INFO	Train Epoch: 225 [0%]
2023-02-18 23:26:26,009	32k	INFO	[2.4124906063079834, 2.2855587005615234, 4.394377708435059, 14.376971244812012, 0.7388967871665955, 11200, 9.720220655606233e-05]
2023-02-18 23:26:47,832	32k	INFO	====> Epoch: 225
2023-02-18 23:27:21,948	32k	INFO	====> Epoch: 226
2023-02-18 23:28:06,092	32k	INFO	====> Epoch: 227
2023-02-18 23:28:40,481	32k	INFO	====> Epoch: 228
2023-02-18 23:28:53,209	32k	INFO	Train Epoch: 229 [0%]
2023-02-18 23:28:53,209	32k	INFO	[2.305823802947998, 2.378619432449341, 5.583840370178223, 19.249460220336914, 0.4263373911380768, 11400, 9.715361456473177e-05]
2023-02-18 23:29:14,482	32k	INFO	====> Epoch: 229
2023-02-18 23:29:48,314	32k	INFO	====> Epoch: 230
2023-02-18 23:30:21,966	32k	INFO	====> Epoch: 231
2023-02-18 23:30:55,698	32k	INFO	====> Epoch: 232
2023-02-18 23:31:08,711	32k	INFO	Train Epoch: 233 [0%]
2023-02-18 23:31:08,712	32k	INFO	[2.481870174407959, 2.143181324005127, 4.262528419494629, 16.171850204467773, 0.7208861708641052, 11600, 9.710504686484176e-05]
2023-02-18 23:31:29,888	32k	INFO	====> Epoch: 233
2023-02-18 23:32:04,047	32k	INFO	====> Epoch: 234
2023-02-18 23:32:38,265	32k	INFO	====> Epoch: 235
2023-02-18 23:33:12,350	32k	INFO	====> Epoch: 236
2023-02-18 23:33:25,272	32k	INFO	Train Epoch: 237 [0%]
2023-02-18 23:33:25,272	32k	INFO	[2.522643804550171, 2.2305104732513428, 4.612065315246582, 17.176006317138672, 0.4836585819721222, 11800, 9.705650344424885e-05]
2023-02-18 23:33:46,476	32k	INFO	====> Epoch: 237
2023-02-18 23:34:21,089	32k	INFO	====> Epoch: 238
2023-02-18 23:34:55,310	32k	INFO	====> Epoch: 239
2023-02-18 23:35:29,230	32k	INFO	====> Epoch: 240
2023-02-18 23:35:41,821	32k	INFO	Train Epoch: 241 [0%]
2023-02-18 23:35:41,821	32k	INFO	[2.4223830699920654, 2.40576171875, 5.261441707611084, 18.649465560913086, 0.7856435775756836, 12000, 9.700798429081568e-05]
2023-02-18 23:35:44,125	32k	INFO	Saving model and optimizer state at iteration 241 to ./logs\32k\G_12000.pth
2023-02-18 23:35:45,930	32k	INFO	Saving model and optimizer state at iteration 241 to ./logs\32k\D_12000.pth
2023-02-18 23:36:17,760	32k	INFO	====> Epoch: 241
2023-02-18 23:36:52,571	32k	INFO	====> Epoch: 242
2023-02-18 23:37:34,882	32k	INFO	====> Epoch: 243
2023-02-18 23:38:09,291	32k	INFO	====> Epoch: 244
2023-02-18 23:38:26,796	32k	INFO	Train Epoch: 245 [0%]
2023-02-18 23:38:26,797	32k	INFO	[2.3705251216888428, 2.3307652473449707, 5.005424499511719, 18.180696487426758, 0.7772977352142334, 12200, 9.695948939241093e-05]
2023-02-18 23:38:48,434	32k	INFO	====> Epoch: 245
2023-02-18 23:39:23,465	32k	INFO	====> Epoch: 246
2023-02-18 23:39:57,151	32k	INFO	====> Epoch: 247
2023-02-18 23:40:30,947	32k	INFO	====> Epoch: 248
2023-02-18 23:40:45,578	32k	INFO	Train Epoch: 249 [0%]
2023-02-18 23:40:45,578	32k	INFO	[2.4981062412261963, 2.2620084285736084, 4.773609638214111, 16.739185333251953, 0.7632009983062744, 12400, 9.691101873690936e-05]
2023-02-18 23:41:07,220	32k	INFO	====> Epoch: 249
2023-02-18 23:41:43,764	32k	INFO	====> Epoch: 250
2023-02-18 23:42:18,389	32k	INFO	====> Epoch: 251
2023-02-18 23:42:53,364	32k	INFO	====> Epoch: 252
2023-02-18 23:43:08,721	32k	INFO	Train Epoch: 253 [0%]
2023-02-18 23:43:08,721	32k	INFO	[2.4519472122192383, 2.336543321609497, 4.729830741882324, 17.976905822753906, 0.64043128490448, 12600, 9.68625723121918e-05]
2023-02-18 23:43:30,444	32k	INFO	====> Epoch: 253
2023-02-18 23:44:10,655	32k	INFO	====> Epoch: 254
2023-02-18 23:44:45,529	32k	INFO	====> Epoch: 255
2023-02-18 23:45:20,366	32k	INFO	====> Epoch: 256
2023-02-18 23:45:33,499	32k	INFO	Train Epoch: 257 [0%]
2023-02-18 23:45:33,499	32k	INFO	[2.537846803665161, 2.301051139831543, 4.906134128570557, 16.203859329223633, 1.0530896186828613, 12800, 9.681415010614512e-05]
2023-02-18 23:45:54,984	32k	INFO	====> Epoch: 257
2023-02-18 23:46:33,593	32k	INFO	====> Epoch: 258
2023-02-18 23:47:16,248	32k	INFO	====> Epoch: 259
2023-02-18 23:47:49,988	32k	INFO	====> Epoch: 260
2023-02-18 23:48:02,450	32k	INFO	Train Epoch: 261 [0%]
2023-02-18 23:48:02,450	32k	INFO	[2.488393783569336, 2.2941668033599854, 4.918285846710205, 19.046873092651367, 0.7982989549636841, 13000, 9.676575210666227e-05]
2023-02-18 23:48:04,718	32k	INFO	Saving model and optimizer state at iteration 261 to ./logs\32k\G_13000.pth
2023-02-18 23:48:06,396	32k	INFO	Saving model and optimizer state at iteration 261 to ./logs\32k\D_13000.pth
2023-02-18 23:48:38,175	32k	INFO	====> Epoch: 261
2023-02-18 23:49:12,225	32k	INFO	====> Epoch: 262
2023-02-18 23:49:46,129	32k	INFO	====> Epoch: 263
2023-02-18 23:50:20,757	32k	INFO	====> Epoch: 264
2023-02-18 23:50:33,204	32k	INFO	Train Epoch: 265 [0%]
2023-02-18 23:50:33,204	32k	INFO	[2.5391273498535156, 2.2686095237731934, 4.443239212036133, 15.730873107910156, 0.5498974323272705, 13200, 9.671737830164223e-05]
2023-02-18 23:50:54,996	32k	INFO	====> Epoch: 265
2023-02-18 23:51:28,909	32k	INFO	====> Epoch: 266
2023-02-18 23:52:02,837	32k	INFO	====> Epoch: 267
2023-02-18 23:52:36,587	32k	INFO	====> Epoch: 268
2023-02-18 23:52:49,100	32k	INFO	Train Epoch: 269 [0%]
2023-02-18 23:52:49,101	32k	INFO	[2.355217456817627, 2.2670557498931885, 5.454554080963135, 19.447181701660156, 0.6202991604804993, 13400, 9.666902867899003e-05]
2023-02-18 23:53:10,368	32k	INFO	====> Epoch: 269
2023-02-18 23:53:44,587	32k	INFO	====> Epoch: 270
2023-02-18 23:54:18,180	32k	INFO	====> Epoch: 271
2023-02-18 23:54:52,279	32k	INFO	====> Epoch: 272
2023-02-18 23:55:04,708	32k	INFO	Train Epoch: 273 [0%]
2023-02-18 23:55:04,708	32k	INFO	[2.341806650161743, 2.3945717811584473, 5.29454231262207, 17.22795295715332, 0.7698819041252136, 13600, 9.662070322661676e-05]
2023-02-18 23:55:26,140	32k	INFO	====> Epoch: 273
2023-02-18 23:55:59,948	32k	INFO	====> Epoch: 274
2023-02-18 23:56:34,252	32k	INFO	====> Epoch: 275
2023-02-18 23:57:07,944	32k	INFO	====> Epoch: 276
2023-02-18 23:57:20,406	32k	INFO	Train Epoch: 277 [0%]
2023-02-18 23:57:20,407	32k	INFO	[2.4298408031463623, 2.173196315765381, 5.081131458282471, 17.086484909057617, 0.7216730117797852, 13800, 9.657240193243954e-05]
2023-02-18 23:57:41,883	32k	INFO	====> Epoch: 277
2023-02-18 23:58:15,909	32k	INFO	====> Epoch: 278
2023-02-18 23:58:50,616	32k	INFO	====> Epoch: 279
2023-02-18 23:59:24,578	32k	INFO	====> Epoch: 280
2023-02-18 23:59:36,819	32k	INFO	Train Epoch: 281 [0%]
2023-02-18 23:59:36,819	32k	INFO	[2.460224151611328, 2.2341036796569824, 5.383091449737549, 18.9133358001709, 0.7073290348052979, 14000, 9.652412478438153e-05]
2023-02-18 23:59:39,070	32k	INFO	Saving model and optimizer state at iteration 281 to ./logs\32k\G_14000.pth
2023-02-18 23:59:40,820	32k	INFO	Saving model and optimizer state at iteration 281 to ./logs\32k\D_14000.pth
2023-02-19 00:00:13,006	32k	INFO	====> Epoch: 281
2023-02-19 00:00:46,880	32k	INFO	====> Epoch: 282
2023-02-19 00:01:20,591	32k	INFO	====> Epoch: 283
2023-02-19 00:01:54,835	32k	INFO	====> Epoch: 284
2023-02-19 00:02:07,162	32k	INFO	Train Epoch: 285 [0%]
2023-02-19 00:02:07,162	32k	INFO	[2.5600364208221436, 2.068615436553955, 4.256036281585693, 14.826904296875, 0.492021769285202, 14200, 9.647587177037196e-05]
2023-02-19 00:02:28,315	32k	INFO	====> Epoch: 285
2023-02-19 00:03:02,279	32k	INFO	====> Epoch: 286
2023-02-19 00:03:35,814	32k	INFO	====> Epoch: 287
2023-02-19 00:04:09,085	32k	INFO	====> Epoch: 288
2023-02-19 00:04:21,477	32k	INFO	Train Epoch: 289 [0%]
2023-02-19 00:04:21,478	32k	INFO	[2.430079936981201, 2.254486560821533, 4.919879913330078, 17.85005760192871, 0.6176902651786804, 14400, 9.642764287834605e-05]
2023-02-19 00:04:42,733	32k	INFO	====> Epoch: 289
2023-02-19 00:05:16,399	32k	INFO	====> Epoch: 290
2023-02-19 00:05:49,993	32k	INFO	====> Epoch: 291
2023-02-19 00:06:23,748	32k	INFO	====> Epoch: 292
2023-02-19 00:06:36,090	32k	INFO	Train Epoch: 293 [0%]
2023-02-19 00:06:36,091	32k	INFO	[2.475820541381836, 2.3945884704589844, 4.486007213592529, 16.214431762695312, 0.5783012509346008, 14600, 9.637943809624507e-05]
2023-02-19 00:06:57,700	32k	INFO	====> Epoch: 293
2023-02-19 00:07:31,220	32k	INFO	====> Epoch: 294
2023-02-19 00:08:06,026	32k	INFO	====> Epoch: 295
2023-02-19 00:08:39,858	32k	INFO	====> Epoch: 296
2023-02-19 00:08:52,224	32k	INFO	Train Epoch: 297 [0%]
2023-02-19 00:08:52,224	32k	INFO	[2.423539638519287, 2.2271597385406494, 4.5769829750061035, 14.893011093139648, 0.6195393800735474, 14800, 9.633125741201631e-05]
2023-02-19 00:09:13,651	32k	INFO	====> Epoch: 297
2023-02-19 00:09:47,161	32k	INFO	====> Epoch: 298
2023-02-19 00:10:20,739	32k	INFO	====> Epoch: 299
2023-02-19 00:10:53,910	32k	INFO	====> Epoch: 300
2023-02-19 00:11:06,740	32k	INFO	Train Epoch: 301 [0%]
2023-02-19 00:11:06,740	32k	INFO	[2.458285093307495, 2.191755771636963, 4.6762824058532715, 16.146800994873047, 0.5016802549362183, 15000, 9.628310081361311e-05]
2023-02-19 00:11:08,985	32k	INFO	Saving model and optimizer state at iteration 301 to ./logs\32k\G_15000.pth
2023-02-19 00:11:10,851	32k	INFO	Saving model and optimizer state at iteration 301 to ./logs\32k\D_15000.pth
2023-02-19 00:11:39,231	32k	INFO	====> Epoch: 301
2023-02-19 00:12:13,076	32k	INFO	====> Epoch: 302
2023-02-19 00:12:46,813	32k	INFO	====> Epoch: 303
2023-02-19 00:13:21,621	32k	INFO	====> Epoch: 304
2023-02-19 00:13:33,931	32k	INFO	Train Epoch: 305 [0%]
2023-02-19 00:13:33,932	32k	INFO	[2.4906296730041504, 2.233870029449463, 4.371082305908203, 15.554403305053711, 0.43802106380462646, 15200, 9.62349682889948e-05]
2023-02-19 00:13:55,628	32k	INFO	====> Epoch: 305
2023-02-19 00:14:28,946	32k	INFO	====> Epoch: 306
2023-02-19 00:15:02,450	32k	INFO	====> Epoch: 307
2023-02-19 00:15:35,915	32k	INFO	====> Epoch: 308
2023-02-19 00:15:48,253	32k	INFO	Train Epoch: 309 [0%]
2023-02-19 00:15:48,253	32k	INFO	[2.4034605026245117, 2.3304038047790527, 4.843508720397949, 16.123014450073242, 0.6900530457496643, 15400, 9.618685982612675e-05]
2023-02-19 00:16:09,955	32k	INFO	====> Epoch: 309
2023-02-19 00:16:43,568	32k	INFO	====> Epoch: 310
2023-02-19 00:17:17,469	32k	INFO	====> Epoch: 311
2023-02-19 00:17:51,151	32k	INFO	====> Epoch: 312
2023-02-19 00:18:03,460	32k	INFO	Train Epoch: 313 [0%]
2023-02-19 00:18:03,460	32k	INFO	[2.398832082748413, 2.109367609024048, 4.755971908569336, 17.07628059387207, 0.714061975479126, 15600, 9.613877541298036e-05]
2023-02-19 00:18:24,958	32k	INFO	====> Epoch: 313
2023-02-19 00:18:58,657	32k	INFO	====> Epoch: 314
2023-02-19 00:19:32,421	32k	INFO	====> Epoch: 315
2023-02-19 00:20:06,541	32k	INFO	====> Epoch: 316
2023-02-19 00:20:18,959	32k	INFO	Train Epoch: 317 [0%]
2023-02-19 00:20:18,959	32k	INFO	[2.580685615539551, 2.222972869873047, 3.95505952835083, 13.909546852111816, 0.6658938527107239, 15800, 9.609071503753299e-05]
2023-02-19 00:20:40,376	32k	INFO	====> Epoch: 317
2023-02-19 00:21:14,430	32k	INFO	====> Epoch: 318
2023-02-19 00:21:48,219	32k	INFO	====> Epoch: 319
2023-02-19 00:22:22,180	32k	INFO	====> Epoch: 320
2023-02-19 00:22:34,738	32k	INFO	Train Epoch: 321 [0%]
2023-02-19 00:22:34,738	32k	INFO	[2.5159249305725098, 2.333894729614258, 4.399147987365723, 16.557025909423828, 0.3425530195236206, 16000, 9.604267868776807e-05]
2023-02-19 00:22:36,937	32k	INFO	Saving model and optimizer state at iteration 321 to ./logs\32k\G_16000.pth
2023-02-19 00:22:38,661	32k	INFO	Saving model and optimizer state at iteration 321 to ./logs\32k\D_16000.pth
2023-02-19 00:23:08,654	32k	INFO	====> Epoch: 321
2023-02-19 00:23:42,708	32k	INFO	====> Epoch: 322
2023-02-19 00:24:16,402	32k	INFO	====> Epoch: 323
2023-02-19 00:24:50,391	32k	INFO	====> Epoch: 324
2023-02-19 00:25:02,735	32k	INFO	Train Epoch: 325 [0%]
2023-02-19 00:25:02,735	32k	INFO	[2.446579933166504, 2.217068672180176, 4.219202518463135, 14.916752815246582, 0.536367654800415, 16200, 9.599466635167497e-05]
2023-02-19 00:25:24,178	32k	INFO	====> Epoch: 325
2023-02-19 00:25:57,696	32k	INFO	====> Epoch: 326
2023-02-19 00:26:31,418	32k	INFO	====> Epoch: 327
2023-02-19 00:27:05,361	32k	INFO	====> Epoch: 328
2023-02-19 00:27:17,871	32k	INFO	Train Epoch: 329 [0%]
2023-02-19 00:27:17,871	32k	INFO	[2.4597926139831543, 2.277541399002075, 5.087441921234131, 17.085941314697266, 0.7290210723876953, 16400, 9.594667801724916e-05]
2023-02-19 00:27:39,736	32k	INFO	====> Epoch: 329
2023-02-19 00:28:14,247	32k	INFO	====> Epoch: 330
2023-02-19 00:28:48,225	32k	INFO	====> Epoch: 331
2023-02-19 00:29:21,925	32k	INFO	====> Epoch: 332
2023-02-19 00:29:34,224	32k	INFO	Train Epoch: 333 [0%]
2023-02-19 00:29:34,224	32k	INFO	[2.463041305541992, 2.3327062129974365, 5.567808628082275, 17.817007064819336, 0.698740541934967, 16600, 9.589871367249203e-05]
2023-02-19 00:29:55,679	32k	INFO	====> Epoch: 333
2023-02-19 00:30:29,245	32k	INFO	====> Epoch: 334
2023-02-19 00:31:02,799	32k	INFO	====> Epoch: 335
2023-02-19 00:31:36,739	32k	INFO	====> Epoch: 336
2023-02-19 00:31:49,149	32k	INFO	Train Epoch: 337 [0%]
2023-02-19 00:31:49,149	32k	INFO	[2.502382278442383, 2.2447524070739746, 4.898643493652344, 16.67108726501465, 0.6746363639831543, 16800, 9.5850773305411e-05]
2023-02-19 00:32:10,876	32k	INFO	====> Epoch: 337
2023-02-19 00:32:44,720	32k	INFO	====> Epoch: 338
2023-02-19 00:33:18,944	32k	INFO	====> Epoch: 339
2023-02-19 00:33:52,799	32k	INFO	====> Epoch: 340
2023-02-19 00:34:05,157	32k	INFO	Train Epoch: 341 [0%]
2023-02-19 00:34:05,158	32k	INFO	[2.397951602935791, 2.365123987197876, 4.729172706604004, 16.68999671936035, 0.4376966059207916, 17000, 9.580285690401946e-05]
2023-02-19 00:34:07,366	32k	INFO	Saving model and optimizer state at iteration 341 to ./logs\32k\G_17000.pth
2023-02-19 00:34:09,007	32k	INFO	Saving model and optimizer state at iteration 341 to ./logs\32k\D_17000.pth
2023-02-19 00:34:40,899	32k	INFO	====> Epoch: 341
2023-02-19 00:35:14,942	32k	INFO	====> Epoch: 342
2023-02-19 00:35:48,658	32k	INFO	====> Epoch: 343
2023-02-19 00:36:22,458	32k	INFO	====> Epoch: 344
2023-02-19 00:36:34,705	32k	INFO	Train Epoch: 345 [0%]
2023-02-19 00:36:34,706	32k	INFO	[2.366065502166748, 2.302611827850342, 5.7743072509765625, 20.71128273010254, 0.6005944013595581, 17200, 9.575496445633683e-05]
2023-02-19 00:36:56,265	32k	INFO	====> Epoch: 345
2023-02-19 00:37:29,855	32k	INFO	====> Epoch: 346
2023-02-19 00:38:03,614	32k	INFO	====> Epoch: 347
2023-02-19 00:38:37,504	32k	INFO	====> Epoch: 348
2023-02-19 00:38:50,051	32k	INFO	Train Epoch: 349 [0%]
2023-02-19 00:38:50,051	32k	INFO	[2.538447380065918, 2.1463463306427, 4.440630912780762, 15.660219192504883, 0.5577057600021362, 17400, 9.570709595038851e-05]
2023-02-19 00:39:11,818	32k	INFO	====> Epoch: 349
2023-02-19 00:39:45,698	32k	INFO	====> Epoch: 350
2023-02-19 00:40:19,789	32k	INFO	====> Epoch: 351
2023-02-19 00:40:54,035	32k	INFO	====> Epoch: 352
2023-02-19 00:41:06,370	32k	INFO	Train Epoch: 353 [0%]
2023-02-19 00:41:06,371	32k	INFO	[2.4533636569976807, 2.280338764190674, 4.7811479568481445, 17.304006576538086, 0.6685123443603516, 17600, 9.565925137420586e-05]
2023-02-19 00:41:27,702	32k	INFO	====> Epoch: 353
2023-02-19 00:42:01,527	32k	INFO	====> Epoch: 354
2023-02-19 00:42:36,230	32k	INFO	====> Epoch: 355
2023-02-19 00:43:09,834	32k	INFO	====> Epoch: 356
2023-02-19 00:43:22,163	32k	INFO	Train Epoch: 357 [0%]
2023-02-19 00:43:22,163	32k	INFO	[2.388760566711426, 2.367077350616455, 5.129391670227051, 18.59344482421875, 0.6342436671257019, 17800, 9.561143071582622e-05]
2023-02-19 00:43:43,628	32k	INFO	====> Epoch: 357
2023-02-19 00:44:17,161	32k	INFO	====> Epoch: 358
2023-02-19 00:44:51,459	32k	INFO	====> Epoch: 359
2023-02-19 00:45:25,593	32k	INFO	====> Epoch: 360
2023-02-19 00:45:37,956	32k	INFO	Train Epoch: 361 [0%]
2023-02-19 00:45:37,957	32k	INFO	[2.4581923484802246, 2.206301689147949, 5.371306419372559, 18.90766716003418, 0.7626745700836182, 18000, 9.556363396329299e-05]
2023-02-19 00:45:40,205	32k	INFO	Saving model and optimizer state at iteration 361 to ./logs\32k\G_18000.pth
2023-02-19 00:45:41,744	32k	INFO	Saving model and optimizer state at iteration 361 to ./logs\32k\D_18000.pth
2023-02-19 00:46:10,121	32k	INFO	====> Epoch: 361
2023-02-19 00:46:43,823	32k	INFO	====> Epoch: 362
2023-02-19 00:47:17,513	32k	INFO	====> Epoch: 363
2023-02-19 00:47:51,660	32k	INFO	====> Epoch: 364
2023-02-19 00:48:04,293	32k	INFO	Train Epoch: 365 [0%]
2023-02-19 00:48:04,293	32k	INFO	[2.4666988849639893, 2.189941644668579, 5.130414009094238, 18.805566787719727, 0.6642502546310425, 18200, 9.551586110465545e-05]
2023-02-19 00:48:26,091	32k	INFO	====> Epoch: 365
2023-02-19 00:48:59,368	32k	INFO	====> Epoch: 366
2023-02-19 00:49:33,105	32k	INFO	====> Epoch: 367
2023-02-19 00:50:06,863	32k	INFO	====> Epoch: 368
2023-02-19 00:50:19,239	32k	INFO	Train Epoch: 369 [0%]
2023-02-19 00:50:19,239	32k	INFO	[2.4212231636047363, 2.2009918689727783, 5.313521862030029, 18.592912673950195, 0.6172853112220764, 18400, 9.546811212796888e-05]
2023-02-19 00:50:40,843	32k	INFO	====> Epoch: 369
2023-02-19 00:51:14,578	32k	INFO	====> Epoch: 370
2023-02-19 00:51:48,321	32k	INFO	====> Epoch: 371
2023-02-19 00:52:21,753	32k	INFO	====> Epoch: 372
2023-02-19 00:52:34,132	32k	INFO	Train Epoch: 373 [0%]
2023-02-19 00:52:34,133	32k	INFO	[2.4616122245788574, 2.2738473415374756, 4.844198703765869, 15.986061096191406, 0.7905173897743225, 18600, 9.542038702129457e-05]
2023-02-19 00:52:55,527	32k	INFO	====> Epoch: 373
2023-02-19 00:53:29,176	32k	INFO	====> Epoch: 374
2023-02-19 00:54:03,089	32k	INFO	====> Epoch: 375
2023-02-19 00:54:36,789	32k	INFO	====> Epoch: 376
2023-02-19 00:54:49,096	32k	INFO	Train Epoch: 377 [0%]
2023-02-19 00:54:49,096	32k	INFO	[2.4370014667510986, 2.2707605361938477, 4.774689197540283, 16.29034996032715, 0.48546579480171204, 18800, 9.537268577269974e-05]
2023-02-19 00:55:10,745	32k	INFO	====> Epoch: 377
2023-02-19 00:55:44,365	32k	INFO	====> Epoch: 378
2023-02-19 00:56:17,735	32k	INFO	====> Epoch: 379
2023-02-19 00:56:51,458	32k	INFO	====> Epoch: 380
2023-02-19 00:57:03,659	32k	INFO	Train Epoch: 381 [0%]
2023-02-19 00:57:03,660	32k	INFO	[2.4226319789886475, 2.2721452713012695, 4.637716770172119, 14.265021324157715, 0.4701876640319824, 19000, 9.532500837025758e-05]
2023-02-19 00:57:05,848	32k	INFO	Saving model and optimizer state at iteration 381 to ./logs\32k\G_19000.pth
2023-02-19 00:57:07,558	32k	INFO	Saving model and optimizer state at iteration 381 to ./logs\32k\D_19000.pth
2023-02-19 00:57:35,064	32k	INFO	====> Epoch: 381
2023-02-19 00:58:08,689	32k	INFO	====> Epoch: 382
2023-02-19 00:58:41,955	32k	INFO	====> Epoch: 383
2023-02-19 00:59:15,516	32k	INFO	====> Epoch: 384
2023-02-19 00:59:27,871	32k	INFO	Train Epoch: 385 [0%]
2023-02-19 00:59:27,872	32k	INFO	[2.6010406017303467, 2.0972375869750977, 4.587593078613281, 14.91818618774414, 0.7768793106079102, 19200, 9.527735480204728e-05]
2023-02-19 00:59:50,471	32k	INFO	====> Epoch: 385
2023-02-19 20:23:15,787	32k	INFO	{'train': {'log_interval': 200, 'eval_interval': 1000, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 12, 'fp16_run': False, 'lr_decay': 0.999875, 'segment_size': 17920, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 384, 'port': '8001'}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 1280, 'hop_length': 320, 'win_length': 1280, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': None}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 256, 'ssl_dim': 256, 'n_speakers': 2}, 'spk': {'dodo': 0}, 'model_dir': './logs\\32k'}
2023-02-19 20:23:36,045	32k	INFO	Loaded checkpoint './logs\32k\G_19000.pth' (iteration 381)
2023-02-19 20:23:40,177	32k	INFO	Loaded checkpoint './logs\32k\D_19000.pth' (iteration 381)
2023-02-19 20:24:33,567	32k	INFO	Train Epoch: 381 [0%]
2023-02-19 20:24:33,567	32k	INFO	[2.4849538803100586, 2.2930448055267334, 4.604424476623535, 17.583982467651367, 0.5678489208221436, 19000, 9.53130927442113e-05]
2023-02-19 20:24:39,298	32k	INFO	Saving model and optimizer state at iteration 381 to ./logs\32k\G_19000.pth
2023-02-19 20:24:41,215	32k	INFO	Saving model and optimizer state at iteration 381 to ./logs\32k\D_19000.pth
2023-02-19 20:25:17,480	32k	INFO	====> Epoch: 381
2023-02-19 20:25:56,338	32k	INFO	====> Epoch: 382
2023-02-19 20:26:34,123	32k	INFO	====> Epoch: 383
2023-02-19 20:27:14,539	32k	INFO	====> Epoch: 384
2023-02-19 20:27:28,700	32k	INFO	Train Epoch: 385 [0%]
2023-02-19 20:27:28,700	32k	INFO	[2.5001044273376465, 2.2153422832489014, 4.391432285308838, 14.225977897644043, 0.711727499961853, 19200, 9.526544513269702e-05]
2023-02-19 20:28:01,277	32k	INFO	====> Epoch: 385
2023-02-19 20:28:49,110	32k	INFO	====> Epoch: 386
2023-02-19 20:29:37,481	32k	INFO	====> Epoch: 387
2023-02-19 20:30:13,961	32k	INFO	====> Epoch: 388
2023-02-19 20:30:27,699	32k	INFO	Train Epoch: 389 [0%]
2023-02-19 20:30:27,700	32k	INFO	[2.3945236206054688, 2.3918232917785645, 5.5386528968811035, 18.981224060058594, 0.42466965317726135, 19400, 9.52178213405219e-05]
2023-02-19 20:30:53,922	32k	INFO	====> Epoch: 389
2023-02-19 20:31:39,769	32k	INFO	====> Epoch: 390
2023-02-19 20:32:26,191	32k	INFO	====> Epoch: 391
2023-02-19 20:33:13,008	32k	INFO	====> Epoch: 392
2023-02-19 20:33:27,042	32k	INFO	Train Epoch: 393 [0%]
2023-02-19 20:33:27,043	32k	INFO	[2.505897283554077, 2.23220157623291, 4.461158275604248, 15.922144889831543, 0.7237078547477722, 19600, 9.517022135577851e-05]
2023-02-19 20:34:00,942	32k	INFO	====> Epoch: 393
2023-02-19 20:34:45,936	32k	INFO	====> Epoch: 394
2023-02-19 20:35:28,645	32k	INFO	====> Epoch: 395
2023-02-19 20:36:16,084	32k	INFO	====> Epoch: 396
2023-02-19 20:36:30,005	32k	INFO	Train Epoch: 397 [0%]
2023-02-19 20:36:30,005	32k	INFO	[2.5329151153564453, 2.2383384704589844, 4.682784557342529, 16.8851375579834, 0.5043936967849731, 19800, 9.512264516656537e-05]
2023-02-19 20:37:03,277	32k	INFO	====> Epoch: 397
2023-02-19 20:37:48,424	32k	INFO	====> Epoch: 398
2023-02-19 20:38:35,738	32k	INFO	====> Epoch: 399
2023-02-19 20:39:23,915	32k	INFO	====> Epoch: 400
2023-02-19 20:39:37,874	32k	INFO	Train Epoch: 401 [0%]
2023-02-19 20:39:37,874	32k	INFO	[2.4116227626800537, 2.3483774662017822, 5.391546726226807, 18.21904754638672, 0.7687338590621948, 20000, 9.507509276098694e-05]
2023-02-19 20:39:40,917	32k	INFO	Saving model and optimizer state at iteration 401 to ./logs\32k\G_20000.pth
2023-02-19 20:39:43,421	32k	INFO	Saving model and optimizer state at iteration 401 to ./logs\32k\D_20000.pth
2023-02-19 20:40:20,937	32k	INFO	====> Epoch: 401
